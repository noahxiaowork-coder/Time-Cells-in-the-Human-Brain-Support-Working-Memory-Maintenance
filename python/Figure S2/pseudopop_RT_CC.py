"""
RT‑based analyses for hippocampal time‑cell data
================================================
This module now contains **two complementary pipelines** operating on the
`neural_data` MATLAB structure generated by `create_neural_data.m`:

1. **`repeated_cv_pseudo_population_rt_fast_slow`** – Monte‑Carlo linear‑SVM
   decoding of *Fast* vs. *Slow* trials (already in v3).
2. **`analyze_rt_fast_slow_correlations`** – Trial‑level mean pairwise
   cross‑correlation analysis, mirroring the previous load‑based function
   but splitting trials by RT quantiles and comparing correlation
   distributions (Fast vs. Slow) with Welch *t*‑tests, box‑plots, KDEs, etc.

Key shared parameters
---------------------
* `quantile` – proportion (e.g. 0.20) of trials assigned to each RT tail.
* `only_correct` – drop incorrect trials before binning.
* `min_trials` – exclude patients with fewer than this many usable trials
  (e.g. those limited to 108).
* `min_time_cells` – minimum # of neurons per patient for CC analysis.
"""

from __future__ import annotations
import numpy as np
import pandas as pd
import scipy.io
import seaborn as sns
from scipy.stats import gaussian_kde
from typing import Iterable, Optional, Dict, List, Tuple
import matplotlib.pyplot as plt
from scipy.stats import ks_2samp
from tqdm import tqdm

# ---------------------------------------------------------------------------
#  Significance helpers
# ---------------------------------------------------------------------------
DIR = '/Users/xiangxuankong/Desktop/Human TC Support WM/Figures/Figure S2/'
def _significance_stars(p: float) -> str:
    if p < 1e-3:
        return "***"
    if p < 1e-2:
        return "**"
    if p < 0.05:
        return "*"
    return "ns"


def add_significance_bar(ax, x1, x2, y, p_value, text_offset=0.02):
    """Draw significance bar with stars above two box‑plot positions."""
    bar_y = y + text_offset
    ax.plot([x1, x1, x2, x2], [y, bar_y, bar_y, y], color="black", linewidth=1)
    ax.text((x1 + x2) / 2, bar_y, _significance_stars(p_value),
            ha="center", va="bottom", fontsize=12)

# ---------------------------------------------------------------------------
#  Trial‑level CC helper (unchanged)
# ---------------------------------------------------------------------------

def process_trial_and_compute_mean_correlation(trial_data: np.ndarray,
                                               time_fields: List[int],
                                               top_percent: int = 100) -> float:
    """Shift each neuron's PSTH to align time‑field bins and return the mean
    of pairwise Pearson correlations within one trial.
    """
    num_neurons, n_bins = trial_data.shape
    valid_idx = [i for i in range(num_neurons) if np.std(trial_data[i]) != 0]
    if len(valid_idx) < 2:
        return 0.0
    data = trial_data[valid_idx]
    fields = [time_fields[i] for i in valid_idx]
    mid = n_bins // 2
    shifted = np.zeros_like(data)
    for i, tf in enumerate(fields):
        shifted[i] = np.roll(data[i], mid - tf)
    corrs = []
    for i in range(len(shifted)):
        for j in range(i + 1, len(shifted)):
            if np.std(shifted[i]) == 0 or np.std(shifted[j]) == 0:
                continue
            v = np.corrcoef(shifted[i], shifted[j])[0, 1]
            if not np.isnan(v):
                corrs.append(v)
    if not corrs:
        return 0.0
    if top_percent < 100:
        k = max(1, int(np.ceil(len(corrs) * top_percent / 100.0)))
        corrs = sorted(corrs, reverse=True)[:k]
    return float(np.mean(corrs))

def separate_trials_by_rt(
    mat_file_path: str,
    patient_id: int,
    *,
    quantile: float = 0.20,
    use_correct: bool = False,
    min_trials: int | None = None
) -> tuple[dict[str, np.ndarray], list[int]] | tuple[None, None]:
    """
    Split one patient’s trials into Fast / Slow RT tails, optionally keeping
    only correct trials, and return:
        ({'fast': arr_F, 'slow': arr_S}, time_fields)

    Parameters
    ----------
    quantile     : lower / upper tail size (e.g. 0.20 ⇒ fastest 20 %, slowest 20 %).
    use_correct  : if True, incorrect trials are removed *after* RT‑tail labelling.
    min_trials   : drop patient if < min_trials usable trials remain overall.
    """
    # ---- load & collect that patient’s neurons --------------------------------
    m        = scipy.io.loadmat(mat_file_path, squeeze_me=True, struct_as_record=False)
    entries  = [e for e in m["neural_data"].flatten()
                if int(e.patient_id) == patient_id]
    if not entries:
        return None, None

    # ---- trial‑level vectors (assume identical length across neurons) ---------
    rt_full   = entries[0].trial_RT.astype(float)
    corr_full = entries[0].trial_correctness.astype(int)

    # mask out NaN RTs first (these trials are unusable everywhere)
    base_mask = ~np.isnan(rt_full)
    if base_mask.sum() < 2:                       # nothing to work with
        return None, None

    # ---- label Fast / Slow on the *full* RT distribution ----------------------
    rt_clean = rt_full[base_mask]
    q_low    = np.quantile(rt_clean,  quantile)
    q_high   = np.quantile(rt_clean, 1 - quantile)

    fast_mask_all = (rt_full <= q_low)   & base_mask
    slow_mask_all = (rt_full >= q_high)  & base_mask

    # ---- optionally drop incorrect trials *after* tail assignment ------------
    if use_correct:
        correct_mask   = (corr_full == 1)
        fast_mask_all &= correct_mask
        slow_mask_all &= correct_mask

    # need at least one trial in each class
    if fast_mask_all.sum() < 1 or slow_mask_all.sum() < 1:
        return None, None

    # enforce minimum total trials if requested
    if min_trials is not None and (fast_mask_all.sum() + slow_mask_all.sum()) < min_trials:
        return None, None

    # ---- build trials × neurons × bins stack ----------------------------------
    fr_stack = np.stack(
        [np.asarray(e.firing_rates) for e in entries],  # shape: trials × neurons_i × bins
        axis=1                                          # → trials × *neurons* × bins
    )

    # sanity‑check consistency
    assert fr_stack.shape[0] == rt_full.shape[0], \
        "Firing‑rate matrices and RT vector must share the same # trials."

    time_fields = [int(e.time_field) - 1 for e in entries]

    rt_dict = {
        "fast": fr_stack[fast_mask_all],
        "slow": fr_stack[slow_mask_all],
    }
    return rt_dict, time_fields

def separate_trials_by_rt_stratified(
    mat_file_path: str,
    patient_id: int,
    *,
    quantile: float = 0.20,
    min_trials: int | None = None
) -> tuple[dict[str, np.ndarray], list[int]] | tuple[None, None]:
    """
    Fast / Slow split is done *within each* Load condition (1, 2, 3),
    using **correct trials only**. Probe in/out is ignored.
    Returns:
        ({'fast': arr_F, 'slow': arr_S}, time_fields)
    """

    # ---------- gather this patient’s trials ----------------------------------
    mat      = scipy.io.loadmat(mat_file_path, squeeze_me=True, struct_as_record=False)
    entries  = [e for e in mat["neural_data"].flatten()
                if int(e.patient_id) == patient_id]
    if not entries:
        return None, None

    rt_full    = entries[0].trial_RT.astype(float)
    corr_full  = entries[0].trial_correctness.astype(int)
    load_full  = entries[0].trial_load.astype(int)  # 1, 2, 3

    # ---- correct-only & finite RT -------------------------------------------
    base_mask = (~np.isnan(rt_full)) & (corr_full == 1)
    if base_mask.sum() < 2:
        return None, None

    # ---- helper: strict fast / slow masks for a sub-condition ---------------
    def subgroup_masks(sub_mask: np.ndarray) -> tuple[np.ndarray, np.ndarray]:
        fast = np.zeros_like(rt_full, dtype=bool)
        slow = np.zeros_like(rt_full, dtype=bool)
        if sub_mask.sum() < 2:
            return fast, slow
        rt_sub = rt_full[sub_mask]
        q_lo   = np.quantile(rt_sub,  quantile)
        q_hi   = np.quantile(rt_sub, 1 - quantile)
        fast[sub_mask] = rt_full[sub_mask] <  q_lo
        slow[sub_mask] = rt_full[sub_mask] >  q_hi
        return fast, slow

    # ---- build global masks by OR-ing the three load conditions ------------
    fast_global = np.zeros_like(rt_full, dtype=bool)
    slow_global = np.zeros_like(rt_full, dtype=bool)

    for load_val in (1, 2, 3):
        sub = base_mask & (load_full == load_val)
        f, s = subgroup_masks(sub)
        fast_global |= f
        slow_global |= s

    # need ≥ 1 trial per class
    if fast_global.sum() < 1 or slow_global.sum() < 1:
        return None, None

    # enforce overall minimum if requested
    if min_trials is not None and (fast_global.sum() + slow_global.sum()) < min_trials:
        return None, None

    # ---- build trials × neurons × bins cube ---------------------------------
    fr_stack = np.stack([np.asarray(e.firing_rates) for e in entries], axis=1)
    assert fr_stack.shape[0] == rt_full.shape[0]

    time_fields = [int(e.time_field) - 1 for e in entries]

    rt_dict = {
        "fast": fr_stack[fast_global],
        "slow": fr_stack[slow_global],
    }
    return rt_dict, time_fields

def analyze_rt_fast_slow_correlations(mat_file_path: str, *,
                                      quantile: float = 0.5,
                                      min_time_cells: int = 5,
                                      top_percent: int = 100,
                                      use_correct: bool = False,
                                      min_trials: int = 0,
                                      random_seed: int = 42,
                                      remove_zero_cc: bool = True,
                                      stratified = True, # Use Correct, and load 1 and load and in and out
                                      verbose: bool = True):
    """Compute and compare mean pairwise correlations for Fast vs. Slow trials.

    Parameters are analogous to the load‑based version.
    Returns two raw lists of correlation values (fast, slow).
    """
    m = scipy.io.loadmat(mat_file_path, squeeze_me=True, struct_as_record=False)
    nd = m["neural_data"].flatten()
    patient_ids = np.unique([int(e.patient_id) for e in nd])

    data_by_patient: Dict[int, Tuple[Dict[str, np.ndarray], List[int]]] = {}
    fast_counts, slow_counts = {}, {}

    iterable = patient_ids
    if verbose:
        iterable = tqdm(iterable, desc="Gathering patients")

    for pid in iterable:
        if stratified:
            rt_dict, t_fields = separate_trials_by_rt_stratified(mat_file_path, pid,
                                                  quantile=quantile,
                                                  min_trials=min_trials)
        else:
            rt_dict, t_fields = separate_trials_by_rt(mat_file_path, pid,
                                                  quantile=quantile,
                                                  use_correct=use_correct,
                                                  min_trials=min_trials)           

        if rt_dict is None:
            continue
        if len(t_fields) < min_time_cells:
            if verbose:
                print(f"Skipping patient {pid}: only {len(t_fields)} time‑cells.")
            continue
        data_by_patient[pid] = (rt_dict, t_fields)
        fast_counts[pid] = rt_dict["fast"].shape[0]
        slow_counts[pid] = rt_dict["slow"].shape[0]

    if not data_by_patient:
        raise RuntimeError("No patients qualified under the filters.")

    # global minimum trial counts per class
    gmin_fast = min(fast_counts.values())
    gmin_slow = min(slow_counts.values())
    if verbose:
        print(f"Global min trials  •  Fast={gmin_fast}  Slow={gmin_slow}")

    rng = np.random.default_rng(random_seed)
    all_cc = {"fast": [], "slow": []}

    iterable = data_by_patient.keys()
    if verbose:
        iterable = tqdm(iterable, desc="Computing CCs")

    for pid in iterable:
        rt_dict, t_fields = data_by_patient[pid]
        # sample equal # of trials per patient per class
        # Use full available trials
        idx_f = rng.choice(rt_dict["fast"].shape[0], gmin_fast, replace=False)
        idx_s = rng.choice(rt_dict["slow"].shape[0], gmin_slow, replace=False)

        for trial in rt_dict["fast"][idx_f]:
            val = process_trial_and_compute_mean_correlation(trial, t_fields, top_percent=top_percent)
            all_cc["fast"].append(val)
        for trial in rt_dict["slow"][idx_s]:
            val = process_trial_and_compute_mean_correlation(trial, t_fields, top_percent=top_percent)
            all_cc["slow"].append(val)

    if remove_zero_cc:
        all_cc["fast"] = [v for v in all_cc["fast"] if v != 0]
        all_cc["slow"] = [v for v in all_cc["slow"] if v != 0]

    if len(all_cc["fast"]) < 2 or len(all_cc["slow"]) < 2:
        print("Too few data points for statistical comparison.")
        return all_cc["fast"], all_cc["slow"]

    # Welch t‑test
    _, p_fast_slow = ks_2samp(all_cc["fast"], all_cc["slow"])
    if verbose:
        print(f"Fast vs Slow  p={p_fast_slow:.4g}")

    # -------------------- plotting ----------------------
    palette = ["royalblue", "firebrick"]
    sns.set_theme(style="whitegrid", context="talk")

    # Box‑plot ------------------------------------------------------------
    df = pd.DataFrame({
        "Cross‑Correlation": all_cc["fast"] + all_cc["slow"],
        "Class": ["Fast"] * len(all_cc["fast"]) + ["Slow"] * len(all_cc["slow"])
    })
    fig, ax = plt.subplots(figsize=(7, 6))
    sns.boxplot(data=df, x="Class", y="Cross‑Correlation", palette=palette,
                width=0.6, showcaps=True, boxprops={"zorder": 2}, showfliers=False,
                whiskerprops={"linewidth": 1}, ax=ax)
    sns.stripplot(data=df, x="Class", y="Cross‑Correlation", color="k",
                  alpha=0.45, size=4, jitter=0.25, zorder=1, ax=ax)
    ax.set_title("Mean Cross‑Correlation by RT Class")
    y_max = df["Cross‑Correlation"].max()
    add_significance_bar(ax, 0, 1, y_max * 1.05, p_fast_slow)
    sns.despine(trim=True)
    plt.tight_layout()
    plt.show()

    # KDE ---------------------------------------------------------------
    fig, ax = plt.subplots(figsize=(7, 6))
    sns.kdeplot(all_cc["fast"], fill=True, label="Fast", color=palette[0], linewidth=1.6, ax=ax)
    sns.kdeplot(all_cc["slow"], fill=True, label="Slow", color=palette[1], linewidth=1.6, ax=ax)
    ax.set_title("Density of Cross‑Correlations (Fast vs Slow)")
    ax.set_xlabel("Mean Pairwise Correlation")
    ax.set_ylabel("Density")
    txt = f"Fast vs Slow: p={p_fast_slow:.3g}"
    ax.text(0.02, 0.97, txt, transform=ax.transAxes,
            ha="left", va="top", fontsize=12,
            bbox=dict(boxstyle="round,pad=0.4", fc="white", ec="0.7"))
    ax.legend(frameon=False)
    sns.despine(trim=True)
    plt.tight_layout()
    plt.show()

    # Peak‑normalised KDE ----------------------------------------------
    fig, ax = plt.subplots(figsize=(7, 6))
    all_vals = np.concatenate([all_cc["fast"], all_cc["slow"]])
    x_grid = np.linspace(all_vals.min(), all_vals.max(), 400)
    for cls, col in zip(["fast", "slow"], palette):
        kde = gaussian_kde(all_cc[cls])
        y = kde(x_grid)
        y /= y.max()
        ax.plot(x_grid, y, color=col, linewidth=1.8, label=cls.capitalize())
        ax.fill_between(x_grid, 0, y, color=col, alpha=0.25)
    ax.set_title("Peak‑Normalised Density (Fast vs Slow)")
    ax.set_xlabel("Mean Pairwise Correlation")
    ax.set_ylabel("Normalised Density")
    ax.set_ylim(0, 1.05)
    ax.legend(frameon=False)
    sns.despine(trim=True)
    plt.tight_layout()
    plt.show()

    # Empirical CDF -----------------------------------------------------
    fig, ax = plt.subplots(figsize=(7, 6))
    for cls, col in zip(["fast", "slow"], palette):
        vals = np.sort(all_cc[cls])
        cdf = np.arange(1, len(vals) + 1) / len(vals)
        ax.plot(vals, cdf, color=col, linewidth=1.8, label=cls.capitalize())
    ax.text(0.02, 0.98, txt, transform=ax.transAxes, ha="left", va="top",
            fontsize=12, bbox=dict(boxstyle="round,pad=0.4", fc="white", ec="0.7"))
    ax.set_title("Cumulative Distribution of Cross-Correlations")
    ax.set_xlabel("Mean Pairwise Correlation")
    ax.set_ylabel("Cumulative Probability")
    ax.set_ylim(0, 1.01)
    ax.legend(frameon=False)
    sns.despine(trim=True)
    plt.tight_layout()
    plt.savefig(DIR + 'CDF.svg', format = 'svg')
    plt.show()

    return all_cc["fast"], all_cc["slow"]

# analyze_rt_fast_slow_correlations('100msTCdata_G2RT_probe_raw.mat')
analyze_rt_fast_slow_correlations('3sig15_data.mat', stratified= True, quantile=0.3, use_correct= True, random_seed=20250710)